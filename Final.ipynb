{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c8ac82-75a2-4c7f-abcc-39681c31e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymysql\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8d2927c8-2eb9-484d-a14e-b2a8a5c7d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_to_tf_mappings = {\n",
    "    \"avg\": \"tf.reduce_mean\",\n",
    "    \"pow\": \"tf.pow\",\n",
    "    \"log\": \"tf.math.log\",\n",
    "    \"greatest\": \"tf.maximum\",\n",
    "    \"exp\": \"tf.exp\",\n",
    "    \"1/1+exp\": \"tf.sigmoid\",  \n",
    "    \"sum*\":\"tf.matmul\",\n",
    "}\n",
    "\n",
    "def extract_view_name(stmt):\n",
    "    if isinstance(stmt, str) and \"create view\" in stmt:\n",
    "        match = re.search(r'create view (\\w+)', stmt)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def extract_select_expressions(stmt):\n",
    "    if isinstance(stmt, str) and (\"select\" in stmt or \"create view\" in stmt):\n",
    "        match = re.search(r'select (.+?) from', stmt)\n",
    "        if match:\n",
    "            return match.group(1).split(' ')\n",
    "    return []\n",
    "\n",
    "def create_tf_expression(stmt):\n",
    "    operators=['+','-','*','/',')','(',',']\n",
    "    functions=sql_to_tf_mappings.keys()\n",
    "    ops=[]\n",
    "    opds=[]\n",
    "    exp=''\n",
    "    for st in stmt:\n",
    "        if st in functions:\n",
    "            exp=exp+sql_to_tf_mappings[st]\n",
    "        elif st in operators:\n",
    "            exp=exp+st\n",
    "        elif st=='as':\n",
    "            break\n",
    "        else:\n",
    "            st=st.split('.')\n",
    "            exp=exp+st[0]\n",
    "    return exp\n",
    "          \n",
    "def end_to_end_translate(sql_statements,features_tablename,target_tablename,iterations,learning_rate,db_host,db_user,db_password,db_name,loss_name):\n",
    "    view_names=[]\n",
    "    expressions=[]\n",
    "    tf_expressions=[]\n",
    "    for sql in sql_statements:\n",
    "        view_names.append(extract_view_name(sql))\n",
    "        exp=extract_select_expressions(sql)\n",
    "        expressions.append(create_tf_expression(exp))\n",
    "        tf_expressions.append(view_names[-1]+\" = \"+expressions[-1])\n",
    "    tf_commands=\"\\n\\t\\t\".join(tf_expressions)\n",
    "    import_statements=f\"\"\"\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \"\"\"\n",
    "    data_statements=f\"\"\"\n",
    "connection = pymysql.connect(host = {db_host},\n",
    "                               port = int(3306),\n",
    "                               user = {db_user},\n",
    "                               password = {db_password},\n",
    "                               db = {db_name})\n",
    "query1 = \"SELECT * from {features_tablename}\"\n",
    "housing_data = pd.read_sql(query1, connection)\n",
    "query2 = \"SELECT * from {target_tablename}\"\n",
    "target_data = pd.read_sql(query2, connection)\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_data,target_data, test_size=0.2, random_state=42)\n",
    "X_train = tf.constant(X_train.values, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train.values, dtype=tf.float32)\n",
    "y_train = tf.reshape(y_train, [-1, 1])\n",
    "weights = tf.Variable(tf.random.normal([X_train.shape[1], 1]), name=\"weights\")\n",
    "bias = tf.Variable(tf.random.normal([1]), name=\"bias\")\n",
    "learning_rate = {learning_rate}\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    \n",
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        {tf_commands}\n",
    "    gradients = tape.gradient(mse, [weights, bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights, bias]))\n",
    "       \n",
    "    if epoch % 100 == 0:\n",
    "       print(f\"Epoch: \",epoch, \"Loss:\",{loss_name}.numpy())\n",
    "    \"\"\"\n",
    "    return import_statements+data_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1cb252bc-b96f-437c-a772-9a866c34578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation=end_to_end_translate(\n",
    "    [\n",
    "        \"create view y_pred as select sum* ( X_train.featurevalue , weights.weight ) + bias.bias as prediction from housing_data as X_train, weights,bias;\",\n",
    "        \"create view squarederror as select pow ( y_pred.prediction - y_train.target , 2 ) as error from y_pred, target_data as y_train;\",\n",
    "        \"create view mse as select avg ( squarederror.error ) from squarederror;\"\n",
    "    ],\n",
    "    features_tablename=\"features\",  \n",
    "    target_tablename=\"targets\",      \n",
    "    iterations=1000,\n",
    "    learning_rate=0.01,\n",
    "    db_host=\"localhost\",\n",
    "    db_user=\"root\",\n",
    "    db_password=\"123456\",\n",
    "    db_name=\"housing_data\",\n",
    "    loss_name=\"mse\"\n",
    ")\n",
    "with open(\"generated_linear.py\", \"w\") as f:\n",
    "    f.write(translation)\n",
    "\n",
    "translation=end_to_end_translate(\n",
    "    [\n",
    "        \"create view logits as select sum* ( X_train.featurevalue , weights.weight ) + bias.bias as error from housing_data as X_train, weights,bias;\",\n",
    "        \"create view y_pred as select 1/1+exp ( logits.error ) as prediction from logits;\",\n",
    "        \"create view bce as select avg ( y_true * log ( y_pred.prediction + 1e-10 ) + (1 - y_train.target ) * log ( 1 - y_pred.prediction + 1e-10 ) ) from target_data as y_train, y_pred;\"\n",
    "    ],\n",
    "    features_tablename=\"features\",  \n",
    "    target_tablename=\"targets\",      \n",
    "    iterations=1000,\n",
    "    learning_rate=0.01,\n",
    "    db_host=\"localhost\",\n",
    "    db_user=\"root\",\n",
    "    db_password=\"123456\",\n",
    "    db_name=\"housing_data\",\n",
    "    loss_name=\"bce\"\n",
    ")\n",
    "with open(\"generated_logistic.py\", \"w\") as f:\n",
    "    f.write(translation)\n",
    "\n",
    "translation=end_to_end_translate(\n",
    "    [\n",
    "        \"create view y_pred as select sum* ( X_train.featurevalue , weights.weight ) + bias.bias as prediction from housing_data as X_train, weights,bias;\",\n",
    "        \"create view y_true as select y_train.target * 2 - 1 from target_data as y_train;\",\n",
    "        \"create view hinge as select avg ( greatest ( 0.0 , 1 - y_true.target * y_pred.prediction ) ) from y_true,y_pred;\"\n",
    "    ],\n",
    "    features_tablename=\"features\",  \n",
    "    target_tablename=\"targets\",      \n",
    "    iterations=1000,\n",
    "    learning_rate=0.01,\n",
    "    db_host=\"localhost\",\n",
    "    db_user=\"root\",\n",
    "    db_password=\"123456\",\n",
    "    db_name=\"housing_data\",\n",
    "    loss_name=\"hinge\"\n",
    ")\n",
    "with open(\"generated_svm.py\", \"w\") as f:\n",
    "    f.write(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4284bedb-8880-4988-9e33-cfe71a98f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "db_name = \"housing_data\"\n",
    "db_host = \"localhost\"\n",
    "db_username = \"root\"\n",
    "db_password = \"123456\"\n",
    "\n",
    "\n",
    "connection = pymysql.connect(host = db_host,\n",
    "                           port = int(3306),\n",
    "                           user = \"root\",\n",
    "                           password = db_password,\n",
    "                           db = db_name)\n",
    "# Load the CSV file into a DataFrame\n",
    "boston_ds = pd.read_csv(\"BostonHousing.csv\")\n",
    "\n",
    "boston_ds=boston_ds.drop(\"medv\", axis = 1)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#cursor.execute(\"create database housing_data;\")\n",
    "#cursor.execute(\"USE housing_data; CREATE TABLE IF NOT EXISTS housing_data (CRIM FLOAT, ZN FLOAT, INDUS FLOAT, CHAS INT, NOX FLOAT, RM FLOAT, AGE FLOAT, DIS FLOAT, RAD INT, TAX FLOAT, PTRATIO FLOAT, B FLOAT, LSTAT FLOAT);\")\n",
    "\n",
    "# Insert each row of data into the MySQL table\n",
    "for _, row in boston_ds.iterrows():\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO housing_data (CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(sql, tuple(row))\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e1a713f6-9561-4224-9847-30a7074ba0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "db_name = \"housing_data\"\n",
    "db_host = \"localhost\"\n",
    "db_username = \"root\"\n",
    "db_password = \"123456\"\n",
    "\n",
    "\n",
    "connection = pymysql.connect(host = db_host,\n",
    "                           port = int(3306),\n",
    "                           user = \"root\",\n",
    "                           password = db_password,\n",
    "                           db = db_name)\n",
    "# Load the CSV file into a DataFrame\n",
    "boston_ds = pd.read_csv(\"BostonHousing.csv\")\n",
    "\n",
    "boston_target=boston_ds[\"medv\"]\n",
    "cursor = connection.cursor()\n",
    "\n",
    "#cursor.execute(\"USE housing_data; CREATE TABLE target_data (target FLOAT);\")\n",
    "\n",
    "for value in boston_target:\n",
    "    sql = \"INSERT INTO target_data (target) VALUES (%s)\"\n",
    "    cursor.execute(sql, (value,))\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "connection.commit()\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "eb15db04-bcc3-43a4-b0e3-37db28ad96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbsva\\AppData\\Local\\Temp\\ipykernel_18296\\3310550168.py:7: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  housing_data = pd.read_sql(query1, connection)\n",
      "C:\\Users\\bbsva\\AppData\\Local\\Temp\\ipykernel_18296\\3310550168.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  target_data = pd.read_sql(query2, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 53127.40625\n",
      "Epoch 100, Loss: 2662.483642578125\n",
      "Epoch 200, Loss: 335.15606689453125\n",
      "Epoch 300, Loss: 250.52830505371094\n",
      "Epoch 400, Loss: 195.07977294921875\n",
      "Epoch 500, Loss: 155.789794921875\n",
      "Epoch 600, Loss: 127.04792785644531\n",
      "Epoch 700, Loss: 105.34964752197266\n",
      "Epoch 800, Loss: 88.73339080810547\n",
      "Epoch 900, Loss: 76.00957489013672\n",
      "Epoch 1000, Loss: 66.32998657226562\n",
      "Epoch 1100, Loss: 59.01860809326172\n",
      "Epoch 1200, Loss: 53.5196533203125\n",
      "Epoch 1300, Loss: 49.38142013549805\n",
      "Epoch 1400, Loss: 46.245914459228516\n",
      "Epoch 1500, Loss: 43.8369140625\n",
      "Epoch 1600, Loss: 41.9465446472168\n",
      "Epoch 1700, Loss: 40.421939849853516\n",
      "Epoch 1800, Loss: 39.15292739868164\n",
      "Epoch 1900, Loss: 38.06157302856445\n",
      "Test MSE: 37.74763107299805\n"
     ]
    }
   ],
   "source": [
    "connection = pymysql.connect(host = db_host,\n",
    "                           port = int(3306),\n",
    "                           user = \"root\",\n",
    "                           password = db_password,\n",
    "                           db = db_name)\n",
    "query1 = \"SELECT * from housing_data\"\n",
    "housing_data = pd.read_sql(query1, connection)\n",
    "query2 = \"SELECT * from target_data\"\n",
    "target_data = pd.read_sql(query2, connection)\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_data,target_data, test_size=0.2, random_state=42)\n",
    "X_train = tf.constant(X_train.values, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train.values, dtype=tf.float32)\n",
    "y_train = tf.reshape(y_train, [-1, 1])\n",
    "weights = tf.Variable(tf.random.normal([X_train.shape[1], 1]), name=\"weights\")\n",
    "bias = tf.Variable(tf.random.normal([1]), name=\"bias\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X_train, weights) + bias\n",
    "        squarederror = tf.pow(y_pred-y_train,2)\n",
    "        mse = tf.reduce_mean(squarederror)\n",
    "    \n",
    "    gradients = tape.gradient(mse, [weights, bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights, bias]))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {mse.numpy()}\")\n",
    "\n",
    "X_test = tf.constant(X_test.values, dtype=tf.float32)\n",
    "y_test = tf.constant(y_test.values, dtype=tf.float32)\n",
    "y_test = tf.reshape(y_test, [-1, 1])\n",
    "\n",
    "y_pred_test=tf.matmul(X_test, weights) + bias\n",
    "#print(y_pred)\n",
    "test_squarederror = tf.square(y_pred_test - y_test)\n",
    "test_mse = tf.reduce_mean(test_squarederror)\n",
    "\n",
    "# Print the test MSE\n",
    "print(f\"Test MSE: {test_mse.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5ff26b-fc9a-4c8a-9e78-c34a784c214a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<string>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:27\u001b[1;36m\u001b[0m\n\u001b[1;33m    squarederror = tf.pow(y_pred-y_train,2)\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "connection = pymysql.connect(host = localhost,\n",
    "                               port = int(3306),\n",
    "                               user = root,\n",
    "                               password = 123456,\n",
    "                               db = housing_data)\n",
    "query1 = \"SELECT * from features\"\n",
    "housing_data = pd.read_sql(query1, connection)\n",
    "query2 = \"SELECT * from target\"\n",
    "target_data = pd.read_sql(query2, connection)\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_data,target_data, test_size=0.2, random_state=42)\n",
    "X_train = tf.constant(X_train.values, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train.values, dtype=tf.float32)\n",
    "y_train = tf.reshape(y_train, [-1, 1])\n",
    "weights = tf.Variable(tf.random.normal([X_train.shape[1], 1]), name=\"weights\")\n",
    "bias = tf.Variable(tf.random.normal([1]), name=\"bias\")\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    \n",
    "for epoch in range(2000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X_train,weights)+bias\n",
    "squarederror = tf.pow(y_pred-y_train,2)\n",
    "mse = tf.reduce_mean(squarederror)\n",
    " = \n",
    "    gradients = tape.gradient(mse, [weights, bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights, bias]))\n",
    "       \n",
    "    if epoch % 100 == 0:\n",
    "       print(f\"Epoch: \",epoch, \"Loss:\",mse.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51f7f7-5287-4aa3-9b2b-b3c684ca2a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
